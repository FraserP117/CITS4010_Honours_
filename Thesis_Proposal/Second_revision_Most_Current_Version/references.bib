@online{FEP-Rough-Guide-Brain,
    author        = {Karl Friston},
    title         = {The free-energy principle: a rough guide to the brain?},
    year          = {2009},
    url           = {https://doi.org/10.1016/j.tics.2009.04.005}
}

@article{BOGACZ2017198,
title = {A tutorial on the free-energy framework for modelling perception and learning},
journal = {Journal of Mathematical Psychology},
volume = {76},
pages = {198-211},
year = {2017},
note = {Model-based Cognitive Neuroscience},
issn = {0022-2496},
doi = {https://doi.org/10.1016/j.jmp.2015.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0022249615000759},
author = {Rafal Bogacz},
abstract = {This paper provides an easy to follow tutorial on the free-energy framework for modelling perception developed by Friston, which extends the predictive coding model of Rao and Ballard. These models assume that the sensory cortex infers the most likely values of attributes or features of sensory stimuli from the noisy inputs encoding the stimuli. Remarkably, these models describe how this inference could be implemented in a network of very simple computational elements, suggesting that this inference could be performed by biological networks of neurons. Furthermore, learning about the parameters describing the features and their uncertainty is implemented in these models by simple rules of synaptic plasticity based on Hebbian learning. This tutorial introduces the free-energy framework using very simple examples, and provides step-by-step derivations of the model. It also discusses in more detail how the model could be implemented in biological neural circuits. In particular, it presents an extended version of the model in which the neurons only sum their inputs, and synaptic plasticity only depends on activity of pre-synaptic and post-synaptic neurons.}
}

@article{BUCKLEY201755,
title = {The free energy principle for action and perception: A mathematical review},
journal = {Journal of Mathematical Psychology},
volume = {81},
pages = {55-79},
year = {2017},
issn = {0022-2496},
doi = {https://doi.org/10.1016/j.jmp.2017.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0022249617300962},
author = {Christopher L. Buckley and Chang Sub Kim and Simon McGregor and Anil K. Seth},
keywords = {Free energy principle, Perception, Action, Inference, Bayesian brain, Agent-based model},
abstract = {The ‘free energy principle’ (FEP) has been suggested to provide a unified theory of the brain, integrating data and theory relating to action, perception, and learning. The theory and implementation of the FEP combines insights from Helmholtzian ‘perception as inference’, machine learning theory, and statistical thermodynamics. Here, we provide a detailed mathematical evaluation of a suggested biologically plausible implementation of the FEP that has been widely used to develop the theory. Our objectives are (i) to describe within a single article the mathematical structure of this implementation of the FEP; (ii) provide a simple but complete agent-based model utilising the FEP and (iii) to disclose the assumption structure of this implementation of the FEP to help elucidate its significance for the brain sciences.}
}

@article{Markovi-2021,
title = {An empirical evaluation of active inference in multi-armed bandits},
journal = {Neural Networks},
volume = {144},
pages = {229-246},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021003233},
author = {Dimitrije Marković and Hrvoje Stojić and Sarah Schwöbel and Stefan J. Kiebel},
keywords = {Decision making, Bayesian inference, Multi-armed bandits, Active inference, Upper confidence bound, Thompson sampling},
abstract = {A key feature of sequential decision making under uncertainty is a need to balance between exploiting—choosing the best action according to the current knowledge, and exploring—obtaining information about values of other actions. The multi-armed bandit problem, a classical task that captures this trade-off, served as a vehicle in machine learning for developing bandit algorithms that proved to be useful in numerous industrial applications. The active inference framework, an approach to sequential decision making recently developed in neuroscience for understanding human and animal behaviour, is distinguished by its sophisticated strategy for resolving the exploration–exploitation trade-off. This makes active inference an exciting alternative to already established bandit algorithms. Here we derive an efficient and scalable approximate active inference algorithm and compare it to two state-of-the-art bandit algorithms: Bayesian upper confidence bound and optimistic Thompson sampling. This comparison is done on two types of bandit problems: a stationary and a dynamic switching bandit. Our empirical evaluation shows that the active inference algorithm does not produce efficient long-term behaviour in stationary bandits. However, in the more challenging switching bandit problem active inference performs substantially better than the two state-of-the-art bandit algorithms. The results open exciting venues for further research in theoretical and applied machine learning, as well as lend additional credibility to active inference as a general framework for studying human and animal behaviour.}
}

@article{Action-Behaviour-FE,
author = {Friston, K  and Daunizeau, J  and Kilner, J  and Kiebel, S},
journal = {Biological cybernetics},
title = {Action and behavior: a free-energy formulation},
year = {2010},
volume = {102},
number = {3},
pages = {227-260},
doi = {10.1007/s00422-010-0364-z},
url = {https://doi.org/10.1007/s00422-010-0364-z}
}


@article{910573,
  author={Forney, G.D.},
  journal={IEEE Transactions on Information Theory}, 
  title={Codes on graphs: normal realizations}, 
  year={2001},
  volume={47},
  number={2},
  pages={520-548},
  doi={10.1109/18.910573}
}


@article{Markov-Blankets-Life,
author = {Kirchhoff, Michael  and Parr, Thomas  and Palacios, Ensor  and Friston, Karl  and Kiverstein, Julian },
title = {The Markov blankets of life: autonomy, active inference and the free energy principle},
journal = {Journal of The Royal Society Interface},
volume = {15},
number = {138},
pages = {20170792},
year = {2018},
doi = {10.1098/rsif.2017.0792},

URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rsif.2017.0792},
eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rsif.2017.0792}
,
    abstract = { This work addresses the autonomous organization of biological systems. It does so by considering the boundaries of biological systems, from individual cells to Home sapiens, in terms of the presence of Markov blankets under the active inference scheme—a corollary of the free energy principle. A Markov blanket defines the boundaries of a system in a statistical sense. Here we consider how a collective of Markov blankets can self-assemble into a global system that itself has a Markov blanket; thereby providing an illustration of how autonomous systems can be understood as having layers of nested and self-sustaining boundaries. This allows us to show that: (i) any living system is a Markov blanketed system and (ii) the boundaries of such systems need not be co-extensive with the biophysical boundaries of a living organism. In other words, autonomous systems are hierarchically composed of Markov blankets of Markov blankets—all the way down to individual cells, all the way up to you and me, and all the way out to include elements of the local environment. }
}


@article{FEP-Math-Review,
title = {The free energy principle for action and perception: A mathematical review},
journal = {Journal of Mathematical Psychology},
volume = {81},
pages = {55-79},
year = {2017},
issn = {0022-2496},
doi = {https://doi.org/10.1016/j.jmp.2017.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0022249617300962},
author = {Christopher L. Buckley and Chang Sub Kim and Simon McGregor and Anil K. Seth},
keywords = {Free energy principle, Perception, Action, Inference, Bayesian brain, Agent-based model},
abstract = {The ‘free energy principle’ (FEP) has been suggested to provide a unified theory of the brain, integrating data and theory relating to action, perception, and learning. The theory and implementation of the FEP combines insights from Helmholtzian ‘perception as inference’, machine learning theory, and statistical thermodynamics. Here, we provide a detailed mathematical evaluation of a suggested biologically plausible implementation of the FEP that has been widely used to develop the theory. Our objectives are (i) to describe within a single article the mathematical structure of this implementation of the FEP; (ii) provide a simple but complete agent-based model utilising the FEP and (iii) to disclose the assumption structure of this implementation of the FEP to help elucidate its significance for the brain sciences.}
}


@Article{Neural-Dynamics-AIF,
AUTHOR = {Da Costa, Lancelot and Parr, Thomas and Sengupta, Biswa and Friston, Karl},
TITLE = {Neural Dynamics under Active Inference: Plausibility and Efficiency of Information Processing},
JOURNAL = {Entropy},
VOLUME = {23},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {454},
URL = {https://www.mdpi.com/1099-4300/23/4/454},
PubMedID = {33921298},
ISSN = {1099-4300},
ABSTRACT = {Active inference is a normative framework for explaining behaviour under the free energy principle—a theory of self-organisation originating in neuroscience. It specifies neuronal dynamics for state-estimation in terms of a descent on (variational) free energy—a measure of the fit between an internal (generative) model and sensory observations. The free energy gradient is a prediction error—plausibly encoded in the average membrane potentials of neuronal populations. Conversely, the expected probability of a state can be expressed in terms of neuronal firing rates. We show that this is consistent with current models of neuronal dynamics and establish face validity by synthesising plausible electrophysiological responses. We then show that these neuronal dynamics approximate natural gradient descent, a well-known optimisation algorithm from information geometry that follows the steepest descent of the objective in information space. We compare the information length of belief updating in both schemes, a measure of the distance travelled in information space that has a direct interpretation in terms of metabolic cost. We show that neural dynamics under active inference are metabolically efficient and suggest that neural representations in biological agents may evolve by approximating steepest descent in information space towards the point of optimal inference.},
DOI = {10.3390/e23040454}
}


@Inbook{Neal1998,
author="Neal, Radford M.
and Hinton, Geoffrey E.",
editor="Jordan, Michael I.",
title="A View of the Em Algorithm that Justifies Incremental, Sparse, and other Variants",
bookTitle="Learning in Graphical Models",
year="1998",
publisher="Springer Netherlands",
address="Dordrecht",
pages="355--368",
abstract="The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible.",
isbn="978-94-011-5014-9",
doi="10.1007/978-94-011-5014-9_12",
url="https://doi.org/10.1007/978-94-011-5014-9_12"
}


@ARTICLE{A-U-Free-Energy,
  
AUTHOR={Feldman, Harriet and Friston, Karl},   
	 
TITLE={Attention, Uncertainty, and Free-Energy},      
	
JOURNAL={Frontiers in Human Neuroscience},      
	
VOLUME={4},           
	
YEAR={2010},      
	  
URL={https://www.frontiersin.org/articles/10.3389/fnhum.2010.00215},       
	
DOI={10.3389/fnhum.2010.00215},      
	
ISSN={1662-5161},   
   
ABSTRACT={We suggested recently that attention can be understood as inferring the level of uncertainty or precision during hierarchical perception. In this paper, we try to substantiate this claim using neuronal simulations of directed spatial attention and biased competition. These simulations assume that neuronal activity encodes a probabilistic representation of the world that optimizes free-energy in a Bayesian fashion. Because free-energy bounds surprise or the (negative) log-evidence for internal models of the world, this optimization can be regarded as evidence accumulation or (generalized) predictive coding. Crucially, both predictions about the state of the world generating sensory data and the precision of those data have to be optimized. Here, we show that if the precision depends on the states, one can explain many aspects of attention. We illustrate this in the context of the Posner paradigm, using the simulations to generate both psychophysical and electrophysiological responses. These simulated responses are consistent with attentional bias or gating, competition for attentional resources, attentional capture and associated speed-accuracy trade-offs. Furthermore, if we present both attended and non-attended stimuli simultaneously, biased competition for neuronal representation emerges as a principled and straightforward property of Bayes-optimal perception.}
}


@article{The-Bayes-Brain,
title = {The Bayesian brain: the role of uncertainty in neural coding and computation},
journal = {Trends in Neurosciences},
volume = {27},
number = {12},
pages = {712-719},
year = {2004},
issn = {0166-2236},
doi = {https://doi.org/10.1016/j.tins.2004.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0166223604003352},
author = {David C. Knill and Alexandre Pouget},
abstract = {To use sensory information efficiently to make judgments and guide action in the world, the brain must represent and use information about uncertainty in its computations for perception and action. Bayesian methods have proven successful in building computational theories for perception and sensorimotor control, and psychophysics is providing a growing body of evidence that human perceptual computations are ‘Bayes' optimal’. This leads to the ‘Bayesian coding hypothesis’: that the brain represents sensory information probabilistically, in the form of probability distributions. Several computational schemes have recently been proposed for how this might be achieved in populations of neurons. Neurophysiological data on the hypothesis, however, is almost non-existent. A major challenge for neuroscientists is to test these ideas experimentally, and so determine whether and how neurons code information about sensory uncertainty.}
}


@article{Bandits,
title = {An empirical evaluation of active inference in multi-armed bandits},
journal = {Neural Networks},
volume = {144},
pages = {229-246},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021003233},
author = {Dimitrije Marković and Hrvoje Stojić and Sarah Schwöbel and Stefan J. Kiebel},
keywords = {Decision making, Bayesian inference, Multi-armed bandits, Active inference, Upper confidence bound, Thompson sampling},
abstract = {A key feature of sequential decision making under uncertainty is a need to balance between exploiting—choosing the best action according to the current knowledge, and exploring—obtaining information about values of other actions. The multi-armed bandit problem, a classical task that captures this trade-off, served as a vehicle in machine learning for developing bandit algorithms that proved to be useful in numerous industrial applications. The active inference framework, an approach to sequential decision making recently developed in neuroscience for understanding human and animal behaviour, is distinguished by its sophisticated strategy for resolving the exploration–exploitation trade-off. This makes active inference an exciting alternative to already established bandit algorithms. Here we derive an efficient and scalable approximate active inference algorithm and compare it to two state-of-the-art bandit algorithms: Bayesian upper confidence bound and optimistic Thompson sampling. This comparison is done on two types of bandit problems: a stationary and a dynamic switching bandit. Our empirical evaluation shows that the active inference algorithm does not produce efficient long-term behaviour in stationary bandits. However, in the more challenging switching bandit problem active inference performs substantially better than the two state-of-the-art bandit algorithms. The results open exciting venues for further research in theoretical and applied machine learning, as well as lend additional credibility to active inference as a general framework for studying human and animal behaviour.}
}

@article{AIF-D,
  author    = {Noor Sajid and
               Philip J. Ball and
               Karl J. Friston},
  title     = {Demystifying active inference},
  journal   = {CoRR},
  volume    = {abs/1909.10863},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.10863},
  eprinttype = {arXiv},
  eprint    = {1909.10863},
  timestamp = {Fri, 27 Sep 2019 13:04:21 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-10863.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@book{Cherniak,
	author = {Christopher Cherniak},
	year = {1986},
	publisher = {MIT Press},
	title = {Minimal Rationality}
}

@book{Newell-Simon,
	author = {Allan Newell and Herbert Simon}, 
	year = {1972},
	title = {Human problem solving},
	publisher = {Prentice-Hall}
}

@online{AFTMT-Problem-Formulation,
        title = {Ep. 27 - Awakening from the Meaning Crisis - Problem Formulation},
        date = {2019},
        organization = {Youtube},
        author = {John Vervaeke},
}

@article{RR-PP,
	title = {Predictive processing and relevance realization: exploring convergent solutions to the frame problem},
	date = {2022},
	journal = {Phenom Cogn Sci},
	url = {https://doi.org/    10.1007/s11097-022-09850-6}
}

@article{RR-Emerging,
    author = {Vervaeke, John and Lillicrap, Timothy P. and Richards, Blake A.},
    title = "{Relevance Realization and the Emerging Framework in Cognitive Science}",
    journal = {Journal of Logic and Computation},
    volume = {22},
    number = {1},
    pages = {79-99},
    year = {2009},
    month = {10},
    abstract = "{We argue that an explanation of relevance realization is a pervasive problem within cognitive science, and that it is becoming the criterion of the cognitive in terms of which a new framework for doing cognitive science is emerging. We articulate that framework and then make use of it to provide the beginnings of a theory of relevance realization that incorporates many existing insights implicit within the contributing disciplines of cognitive science. We also introduce some theoretical and potentially technical innovations motivated by the articulation of those insights. Finally, we show how the explication of the framework and development of the theory help to clear up some important incompleteness and confusions within both Montague's work and Sperber and Wilson's theory of relevance.}",
    issn = {0955-792X},
    doi = {10.1093/logcom/exp067},
    url = {https://doi.org/10.1093/logcom/exp067},
    eprint = {https://academic.oup.com/logcom/article-pdf/22/1/79/3262477/exp067.pdf},
}

@article{Silver2017,
author={Silver, David
and Schrittwieser, Julian
and Simonyan, Karen
and Antonoglou, Ioannis
and Huang, Aja
and Guez, Arthur
and Hubert, Thomas
and Baker, Lucas
and Lai, Matthew
and Bolton, Adrian
and Chen, Yutian
and Lillicrap, Timothy
and Hui, Fan
and Sifre, Laurent
and van den Driessche, George
and Graepel, Thore
and Hassabis, Demis},
title={Mastering the game of Go without human knowledge},
journal={Nature},
year={2017},
month={10},
volume={550},
number={7676},
pages={354-359},
abstract={A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100--0 against the previously published, champion-defeating AlphaGo.},
issn={1476-4687},
doi={10.1038/nature24270},
url={https://doi.org/10.1038/nature24270}
}

@misc{dream,
      title={Dream to Control: Learning Behaviors by Latent Imagination}, 
      author={Danijar Hafner and Timothy Lillicrap and Jimmy Ba and Mohammad Norouzi},
      year={2020},
      eprint={1912.01603},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{VI,
	doi = {10.1080/01621459.2017.1285773},
  
  
	year = 2017,
	month = {04},
  
	publisher = {Informa {UK} Limited},
  
	volume = {112},
  
	number = {518},
  
	pages = {859--877},
  
	author = {David M. Blei and Alp Kucukelbir and Jon D. McAuliffe},
  
	title = {Variational Inference: A Review for Statisticians},
  
	journal = {Journal of the American Statistical Association}
}


@article{AIF-Cur-Insight,
author = {Friston, Karl and Lin, Marco and Frith, Chris and Pezzulo, Giovanni and Hobson, J. and Ondobaka, Sasha},
year = {2017},
month = {08},
pages = {1-51},
title = {Active Inference, Curiosity and Insight},
volume = {29},
journal = {Neural Computation},
doi = {10.1162/neco_a_00999}
}


@article{RL-or-AIF,
    doi = {10.1371/journal.pone.0006421},
    author = {Friston, Karl J. AND Daunizeau, Jean AND Kiebel, Stefan J.},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Reinforcement Learning or Active Inference?},
    year = {2009},
    month = {07},
    volume = {4},
    url = {https://doi.org/10.1371/journal.pone.0006421},
    pages = {1-13},
    abstract = {This paper questions the need for reinforcement learning or control theory when optimising behaviour. We show that it is fairly simple to teach an agent complicated and adaptive behaviours using a free-energy formulation of perception. In this formulation, agents adjust their internal states and sampling of the environment to minimize their free-energy. Such agents learn causal structure in the environment and sample it in an adaptive and self-supervised fashion. This results in behavioural policies that reproduce those optimised by reinforcement learning and dynamic programming. Critically, we do not need to invoke the notion of reward, value or utility. We illustrate these points by solving a benchmark problem in dynamic programming; namely the mountain-car problem, using active perception or inference under the free-energy principle. The ensuing proof-of-concept may be important because the free-energy formulation furnishes a unified account of both action and perception and may speak to a reappraisal of the role of dopamine in the brain.},
    number = {7},

}


@Article{Friston2012,
author={Friston, Karl
and Samothrakis, Spyridon
and Montague, Read},
title={Active inference and agency: optimal control without cost functions},
journal={Biological Cybernetics},
year={2012},
month={10},
day={01},
volume={106},
number={8},
pages={523-541},
abstract={This paper describes a variational free-energy formulation of (partially observable) Markov decision problems in decision making under uncertainty. We show that optimal control can be cast as active inference. In active inference, both action and posterior beliefs about hidden states minimise a free energy bound on the negative log-likelihood of observed states, under a generative model. In this setting, reward or cost functions are absorbed into prior beliefs about state transitions and terminal states. Effectively, this converts optimal control into a pure inference problem, enabling the application of standard Bayesian filtering techniques. We then consider optimal trajectories that rest on posterior beliefs about hidden states in the future. Crucially, this entails modelling control as a hidden state that endows the generative model with a representation of agency. This leads to a distinction between models with and without inference on hidden control states; namely, agency-free and agency-based models, respectively.},
issn={1432-0770},
doi={10.1007/s00422-012-0512-8},
url={https://doi.org/10.1007/s00422-012-0512-8}
}


@article{Optim-Motor,
title = {What Is Optimal about Motor Control?},
journal = {Neuron},
volume = {72},
number = {3},
pages = {488-498},
year = {2011},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2011.10.018},
url = {https://www.sciencedirect.com/science/article/pii/S0896627311009305},
author = {Karl Friston},
abstract = {This article poses a controversial question: is optimal control theory useful for understanding motor behavior or is it a misdirection? This question is becoming acute as people start to conflate internal models in motor control and perception (Poeppel et al., 2008, Hickok et al., 2011). However, the forward models in motor control are not the generative models used in perceptual inference. This Perspective tries to highlight the differences between internal models in motor control and perception and asks whether optimal control is the right way to think about things. The issues considered here may have broader implications for optimal decision theory and Bayesian approaches to learning and behavior in general.}
}


@ARTICLE{Sim-AIF-Message,
  
AUTHOR={van de Laar, Thijs W. and de Vries, Bert},   
	 
TITLE={Simulating Active Inference Processes by Message Passing},      
	
JOURNAL={Frontiers in Robotics and AI},      
	
VOLUME={6},           
	
YEAR={2019},      
	  
URL={https://www.frontiersin.org/articles/10.3389/frobt.2019.00020},       
	
DOI={10.3389/frobt.2019.00020},      
	
ISSN={2296-9144},   
   
ABSTRACT={The free energy principle (FEP) offers a variational calculus-based description for how biological agents persevere through interactions with their environment. Active inference (AI) is a corollary of the FEP, which states that biological agents act to fulfill prior beliefs about preferred future observations (target priors). Purposeful behavior then results from variational free energy minimization with respect to a generative model of the environment with included target priors. However, manual derivations for free energy minimizing algorithms on custom dynamic models can become tedious and error-prone. While probabilistic programming (PP) techniques enable automatic derivation of inference algorithms on free-form models, full automation of AI requires specialized tools for inference on dynamic models, together with the description of an experimental protocol that governs the interaction between the agent and its simulated environment. The contributions of the present paper are two-fold. Firstly, we illustrate how AI can be automated with the use of ForneyLab, a recent PP toolbox that specializes in variational inference on flexibly definable dynamic models. More specifically, we describe AI agents in a dynamic environment as probabilistic state space models (SSM) and perform inference for perception and control in these agents by message passing on a factor graph representation of the SSM. Secondly, we propose a formal experimental protocol for simulated AI. We exemplify how this protocol leads to goal-directed behavior for flexibly definable AI agents in two classical RL examples, namely the Bayesian thermostat and the mountain car parking problems.}
}


@article{Cox-2019,
	doi = {10.1016/j.ijar.2018.11.002},
  
  
	year = 2019,
	month = {01},
  
	publisher = {Elsevier {BV}},
  
	volume = {104},
  
	pages = {185--204},
  
	author = {Marco Cox and Thijs van de Laar and Bert de Vries},
  
	title = {A factor graph approach to automated design of Bayesian signal processing algorithms},
  
	journal = {International Journal of Approximate Reasoning}
}


@article{Reactive-MP,
  author    = {Dmitry Bagaev and
               Bert de Vries},
  title     = {Reactive Message Passing for Scalable Bayesian Inference},
  journal   = {CoRR},
  volume    = {abs/2112.13251},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.13251},
  eprinttype = {arXiv},
  eprint    = {2112.13251},
  timestamp = {Tue, 04 Jan 2022 15:59:27 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-13251.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@ARTICLE{Deep-Temp-AIF,
  
AUTHOR={de Vries, Bert and Friston, Karl J.},   
	 
TITLE={A Factor Graph Description of Deep Temporal Active Inference},      
	
JOURNAL={Frontiers in Computational Neuroscience},      
	
VOLUME={11},           
	
YEAR={2017},      
	  
URL={https://www.frontiersin.org/articles/10.3389/fncom.2017.00095},       
	
DOI={10.3389/fncom.2017.00095},      
	
ISSN={1662-5188},   
   
ABSTRACT={Active inference is a corollary of the Free Energy Principle that prescribes how self-organizing biological agents interact with their environment. The study of active inference processes relies on the definition of a generative probabilistic model and a description of how a free energy functional is minimized by neuronal message passing under that model. This paper presents a tutorial introduction to specifying active inference processes by Forney-style factor graphs (FFG). The FFG framework provides both an insightful representation of the probabilistic model and a biologically plausible inference scheme that, in principle, can be automatically executed in a computer simulation. As an illustrative example, we present an FFG for a deep temporal active inference process. The graph clearly shows how policy selection by expected free energy minimization results from free energy minimization per se, in an appropriate generative policy model.}
}

@article{Deep-AIF-Ueltzh-2018,
	doi = {10.1007/s00422-018-0785-7},
  
	howpublished = {\url{https://doi.org/10.1007\%2Fs00422-018-0785-7}},
  
	year = 2018,
	month = {10},
  
	publisher = {Springer Science and Business Media {LLC}},
  
	volume = {112},
  
	number = {6},
  
	pages = {547--573},
  
	author = {Kai Ueltzhöffer},
  
	title = {Deep active inference},
  
	journal = {Biological Cybernetics}
}

@article{Deep-Var-Policy-Grad,
author = {Millidge, Beren},
year = {2019},
month = {07},
title = {Deep Active Inference as Variational Policy Gradients},
url = {https://arxiv.org/pdf/1907.03876.pdf}
}

@article{DEEP-AIF-POMDPs,
  author    = {Otto van der Himst and
               Pablo Lanillos},
  title     = {Deep Active Inference for Partially Observable MDPs},
  journal   = {CoRR},
  volume    = {abs/2009.03622},
  year      = {2020},
  url       = {https://arxiv.org/abs/2009.03622},
  eprinttype = {arXiv},
  eprint    = {2009.03622},
  timestamp = {Thu, 17 Sep 2020 12:49:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2009-03622.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Scaling-AIF,
  author    = {Alexander Tschantz and
               Manuel Baltieri and
               Anil K. Seth and
               Christopher L. Buckley},
  title     = {Scaling active inference},
  journal   = {CoRR},
  volume    = {abs/1911.10601},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.10601},
  eprinttype = {arXiv},
  eprint    = {1911.10601},
  timestamp = {Tue, 03 Dec 2019 14:15:54 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-10601.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Contrastive-AIF,
 author = {Mazzaglia, Pietro and Verbelen, Tim and Dhoedt, Bart},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {13870--13882},
 publisher = {Curran Associates, Inc.},
 title = {Contrastive Active Inference},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/73c730319cf839f143bf40954448ce39-Paper.pdf},
 volume = {34},
 year = {2021}
}

@software{minigrid,
  author = {Chevalier-Boisvert, Maxime and Willems, Lucas and Pal, Suman},
  title = {Minimalistic Gridworld Environment for Gymnasium},
  url = {https://github.com/Farama-Foundation/Minigrid},
  year = {2018},
}

@article{Deep-Mind-Control-Suite,
         title = {dm_control: Software and tasks for continuous control},
         journal = {Software Impacts},
         volume = {6},
         pages = {100022},
         year = {2020},
         issn = {2665-9638},
         doi = {https://doi.org/10.1016/j.simpa.2020.100022},
         url = {https://www.sciencedirect.com/science/article/pii/S2665963820300099},
         author = {Saran Tunyasuvunakool and Alistair Muldal and Yotam Doron and
                   Siqi Liu and Steven Bohez and Josh Merel and Tom Erez and
                   Timothy Lillicrap and Nicolas Heess and Yuval Tassa},
}

@online{RxInfer,

}
