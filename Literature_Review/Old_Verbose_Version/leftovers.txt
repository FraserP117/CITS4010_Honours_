
The question concertning how an intelligent agent makes optimal inferences about the environment in which it is situated, and the related task, pertaining to the means by which such an agent performs the optimal actions in this environment, is one of crucial importance for both the theoretical attempt to provide a functional account of intelligent behaviour, and the practical endevour, concerend with controlling and optimising many real-world systems. 

A great many paradigms exist as attmepts to provide an account of the above questions. The theory of ``Active Infernce'' is a comparativley recent, biologically inspired account of intelligent behaviour, pertaining to tasks where it is necessary to act under uncertainty. Active Inference - hencefourth ``AIF'' - is a product of the attmpt to synopticaly integrate functional accounts of intelligent behaviour from Cognitive Science, Neuroscience, Artificial Intelligence and Statistical Mechanics, see: \textcite{FEP-Math-Review}, \textcite{Neural-Dynamics-AIF} and \textcite{Neal1998}.

Indeed, AIF is a corollary of the Free Energy Principle (FEP) in Physics and Biology, see: \textcite{FEP_Rough_Guide_Brain}. The FEP itself, purports to offer a unified account of the imperatives native to any system that resists a thermodynamic tendency to dissipate with time, see: \textcite{Markov-Blankets-Life} for more on the FEP (proper).

While some of the high-level ideas present in AIF have been known for deccades - Free Energy, Approximate Bayesian and Variational Inference for example - the actual use of AIF in Artificial Intelligence and Machine Learning applications has been historically hindered by the absence of a clear, unifying account as to what these methods encode in the action-perception loop. Worse still, the actual computation of an approximate bayesian posterior is very often intractable, in the general case, not to mention the true posterior.

Various sampling based methods like MCMC, typically eschew this issue of computational intractablity, but at the cost of efficiency: citation needed. Indeed, owing to the rigirous asymptotic bounds that exist for MCMC, it has proven to be a popular choice for implementing approximate bayesian inference, especially given the ever-expanding repitore of powerful hardware, such as the GPU, thereby offsetting its comparative inefficency. 

As against the large, inneficent sampling methods like MCMC there still stand the small, efficent variational methods like Active Inference. To be sure, either pole on this spectrum makes a trade-off with respect to efficency, tractability and the presence or absence of asymptotic convergence guarantees, so it may seem that we have no \textit{a priori} reason to suppose that one is more worthy of investigation - insofar as the investigation is an attempt to discover the most plausible account of intelligent behaviour, in terms of the idealised mathematical approximations that Artificial Intelligence and Machine Learning affords. I think it is highly plausible that we do have a reason to ``favour'' the variational methods over the sampling methods (in this respect), for the very reason that there exists an abundence of plausibility arguments that the brain appears to be implementing Active Inference, and not MCMC. See: \textcite{Neural-Dynamics-AIF}, \textcite{A-U-Free-Energy}, \textcite{The-Bayes-Brain} and \textcite{Action-Behaviour-FE}. Insofar as it is the prerogative of Artificial Intelligence to take inspiation from natural, biological phenomena, I think there is good reason to investigate Active Infernce on this account. 

There have been great advances in the attempt to specify ever more tractible approximations to the true bayesian posterior and the future appears hopeful on this front, see ``Neuronal message passing using Mean-field, Bethe, and Marginal approximations'', ``A Factor Graph Approach to Automated Design of
Bayesian Signal Processing Algorithms'' and ``Simulating Active Inference Processes by Message Passing''

Owing to AIF's ability to deal with dynamic uncertainty, it is therfore plausible that the Method provides a robust means of optimal inference, state-estimation, planning, learning andcontrol - and perhaps to a greater extent than the state of the art in reinforcement learning, LQG control and bayesian upper-confidence bound. 

The aim of this thesis wil be to assess the degree to which AIF models consitute robust means of optimal inference and control in simple, noisy, stationary and non-stationary environments. The environments of interest are to be the simple benchmark control environments: mountaincar and cartpole, allong with at least one game-playing environment.     

