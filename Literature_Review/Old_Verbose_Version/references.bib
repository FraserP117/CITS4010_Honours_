@online{FEP_Rough_Guide_Brain,
    author        = {Karl Friston},
    title         = {The free-energy principle: a rough guide to the brain?},
    year          = {2009},
    url           = {https://www.fil.ion.ucl.ac.uk/~karl/The%20free-energy%20principle%20-%20a%20rough%20guide%20to%20the%20brain.pdf}
}

@article{BOGACZ2017198,
title = {A tutorial on the free-energy framework for modelling perception and learning},
journal = {Journal of Mathematical Psychology},
volume = {76},
pages = {198-211},
year = {2017},
note = {Model-based Cognitive Neuroscience},
issn = {0022-2496},
doi = {https://doi.org/10.1016/j.jmp.2015.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0022249615000759},
author = {Rafal Bogacz},
abstract = {This paper provides an easy to follow tutorial on the free-energy framework for modelling perception developed by Friston, which extends the predictive coding model of Rao and Ballard. These models assume that the sensory cortex infers the most likely values of attributes or features of sensory stimuli from the noisy inputs encoding the stimuli. Remarkably, these models describe how this inference could be implemented in a network of very simple computational elements, suggesting that this inference could be performed by biological networks of neurons. Furthermore, learning about the parameters describing the features and their uncertainty is implemented in these models by simple rules of synaptic plasticity based on Hebbian learning. This tutorial introduces the free-energy framework using very simple examples, and provides step-by-step derivations of the model. It also discusses in more detail how the model could be implemented in biological neural circuits. In particular, it presents an extended version of the model in which the neurons only sum their inputs, and synaptic plasticity only depends on activity of pre-synaptic and post-synaptic neurons.}
}

@article{BUCKLEY201755,
title = {The free energy principle for action and perception: A mathematical review},
journal = {Journal of Mathematical Psychology},
volume = {81},
pages = {55-79},
year = {2017},
issn = {0022-2496},
doi = {https://doi.org/10.1016/j.jmp.2017.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0022249617300962},
author = {Christopher L. Buckley and Chang Sub Kim and Simon McGregor and Anil K. Seth},
keywords = {Free energy principle, Perception, Action, Inference, Bayesian brain, Agent-based model},
abstract = {The ‘free energy principle’ (FEP) has been suggested to provide a unified theory of the brain, integrating data and theory relating to action, perception, and learning. The theory and implementation of the FEP combines insights from Helmholtzian ‘perception as inference’, machine learning theory, and statistical thermodynamics. Here, we provide a detailed mathematical evaluation of a suggested biologically plausible implementation of the FEP that has been widely used to develop the theory. Our objectives are (i) to describe within a single article the mathematical structure of this implementation of the FEP; (ii) provide a simple but complete agent-based model utilising the FEP and (iii) to disclose the assumption structure of this implementation of the FEP to help elucidate its significance for the brain sciences.}
}

@article{Markovi__2021,
	doi = {10.1016/j.neunet.2021.08.018},
  
	url = {https://doi.org/10.1016%2Fj.neunet.2021.08.018},
  
	year = 2021,
	month = {dec},
  
	publisher = {Elsevier {BV}},
  
	volume = {144},
  
	pages = {229--246},
  
	author = {Dimitrije Markovi{\'{c}} and Hrvoje Stoji{\'{c}} and Sarah Schwöbel and Stefan J. Kiebel},
  
	title = {An empirical evaluation of active inference in multi-armed bandits},
  
	journal = {Neural Networks}
}

@article{Action-Behaviour-FE,
author = {Friston, K  and Daunizeau, J  and Kilner, J  and Kiebel, S},
journal = {Biological cybernetics},
title = {Action and behavior: a free-energy formulation},
year = {2010},
volume = {102},
number = {3},
pages = {227-260},
doi = {10.1007/s00422-010-0364-z},
url = {https://doi.org/10.1007/s00422-010-0364-z}
}


@article{910573,
  author={Forney, G.D.},
  journal={IEEE Transactions on Information Theory}, 
  title={Codes on graphs: normal realizations}, 
  year={2001},
  volume={47},
  number={2},
  pages={520-548},
  doi={10.1109/18.910573}
}


@article{Markov-Blankets-Life,
author = {Kirchhoff, Michael  and Parr, Thomas  and Palacios, Ensor  and Friston, Karl  and Kiverstein, Julian },
title = {The Markov blankets of life: autonomy, active inference and the free energy principle},
journal = {Journal of The Royal Society Interface},
volume = {15},
number = {138},
pages = {20170792},
year = {2018},
doi = {10.1098/rsif.2017.0792},

URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rsif.2017.0792},
eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rsif.2017.0792}
,
    abstract = { This work addresses the autonomous organization of biological systems. It does so by considering the boundaries of biological systems, from individual cells to Home sapiens, in terms of the presence of Markov blankets under the active inference scheme—a corollary of the free energy principle. A Markov blanket defines the boundaries of a system in a statistical sense. Here we consider how a collective of Markov blankets can self-assemble into a global system that itself has a Markov blanket; thereby providing an illustration of how autonomous systems can be understood as having layers of nested and self-sustaining boundaries. This allows us to show that: (i) any living system is a Markov blanketed system and (ii) the boundaries of such systems need not be co-extensive with the biophysical boundaries of a living organism. In other words, autonomous systems are hierarchically composed of Markov blankets of Markov blankets—all the way down to individual cells, all the way up to you and me, and all the way out to include elements of the local environment. }
}


@article{FEP-Math-Review,
title = {The free energy principle for action and perception: A mathematical review},
journal = {Journal of Mathematical Psychology},
volume = {81},
pages = {55-79},
year = {2017},
issn = {0022-2496},
doi = {https://doi.org/10.1016/j.jmp.2017.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0022249617300962},
author = {Christopher L. Buckley and Chang Sub Kim and Simon McGregor and Anil K. Seth},
keywords = {Free energy principle, Perception, Action, Inference, Bayesian brain, Agent-based model},
abstract = {The ‘free energy principle’ (FEP) has been suggested to provide a unified theory of the brain, integrating data and theory relating to action, perception, and learning. The theory and implementation of the FEP combines insights from Helmholtzian ‘perception as inference’, machine learning theory, and statistical thermodynamics. Here, we provide a detailed mathematical evaluation of a suggested biologically plausible implementation of the FEP that has been widely used to develop the theory. Our objectives are (i) to describe within a single article the mathematical structure of this implementation of the FEP; (ii) provide a simple but complete agent-based model utilising the FEP and (iii) to disclose the assumption structure of this implementation of the FEP to help elucidate its significance for the brain sciences.}
}


@Article{Neural-Dynamics-AIF,
AUTHOR = {Da Costa, Lancelot and Parr, Thomas and Sengupta, Biswa and Friston, Karl},
TITLE = {Neural Dynamics under Active Inference: Plausibility and Efficiency of Information Processing},
JOURNAL = {Entropy},
VOLUME = {23},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {454},
URL = {https://www.mdpi.com/1099-4300/23/4/454},
PubMedID = {33921298},
ISSN = {1099-4300},
ABSTRACT = {Active inference is a normative framework for explaining behaviour under the free energy principle—a theory of self-organisation originating in neuroscience. It specifies neuronal dynamics for state-estimation in terms of a descent on (variational) free energy—a measure of the fit between an internal (generative) model and sensory observations. The free energy gradient is a prediction error—plausibly encoded in the average membrane potentials of neuronal populations. Conversely, the expected probability of a state can be expressed in terms of neuronal firing rates. We show that this is consistent with current models of neuronal dynamics and establish face validity by synthesising plausible electrophysiological responses. We then show that these neuronal dynamics approximate natural gradient descent, a well-known optimisation algorithm from information geometry that follows the steepest descent of the objective in information space. We compare the information length of belief updating in both schemes, a measure of the distance travelled in information space that has a direct interpretation in terms of metabolic cost. We show that neural dynamics under active inference are metabolically efficient and suggest that neural representations in biological agents may evolve by approximating steepest descent in information space towards the point of optimal inference.},
DOI = {10.3390/e23040454}
}


@Inbook{Neal1998,
author="Neal, Radford M.
and Hinton, Geoffrey E.",
editor="Jordan, Michael I.",
title="A View of the Em Algorithm that Justifies Incremental, Sparse, and other Variants",
bookTitle="Learning in Graphical Models",
year="1998",
publisher="Springer Netherlands",
address="Dordrecht",
pages="355--368",
abstract="The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible.",
isbn="978-94-011-5014-9",
doi="10.1007/978-94-011-5014-9_12",
url="https://doi.org/10.1007/978-94-011-5014-9_12"
}


@ARTICLE{A-U-Free-Energy,
  
AUTHOR={Feldman, Harriet and Friston, Karl},   
	 
TITLE={Attention, Uncertainty, and Free-Energy},      
	
JOURNAL={Frontiers in Human Neuroscience},      
	
VOLUME={4},           
	
YEAR={2010},      
	  
URL={https://www.frontiersin.org/articles/10.3389/fnhum.2010.00215},       
	
DOI={10.3389/fnhum.2010.00215},      
	
ISSN={1662-5161},   
   
ABSTRACT={We suggested recently that attention can be understood as inferring the level of uncertainty or precision during hierarchical perception. In this paper, we try to substantiate this claim using neuronal simulations of directed spatial attention and biased competition. These simulations assume that neuronal activity encodes a probabilistic representation of the world that optimizes free-energy in a Bayesian fashion. Because free-energy bounds surprise or the (negative) log-evidence for internal models of the world, this optimization can be regarded as evidence accumulation or (generalized) predictive coding. Crucially, both predictions about the state of the world generating sensory data and the precision of those data have to be optimized. Here, we show that if the precision depends on the states, one can explain many aspects of attention. We illustrate this in the context of the Posner paradigm, using the simulations to generate both psychophysical and electrophysiological responses. These simulated responses are consistent with attentional bias or gating, competition for attentional resources, attentional capture and associated speed-accuracy trade-offs. Furthermore, if we present both attended and non-attended stimuli simultaneously, biased competition for neuronal representation emerges as a principled and straightforward property of Bayes-optimal perception.}
}


@article{The-Bayes-Brain,
title = {The Bayesian brain: the role of uncertainty in neural coding and computation},
journal = {Trends in Neurosciences},
volume = {27},
number = {12},
pages = {712-719},
year = {2004},
issn = {0166-2236},
doi = {https://doi.org/10.1016/j.tins.2004.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0166223604003352},
author = {David C. Knill and Alexandre Pouget},
abstract = {To use sensory information efficiently to make judgments and guide action in the world, the brain must represent and use information about uncertainty in its computations for perception and action. Bayesian methods have proven successful in building computational theories for perception and sensorimotor control, and psychophysics is providing a growing body of evidence that human perceptual computations are ‘Bayes' optimal’. This leads to the ‘Bayesian coding hypothesis’: that the brain represents sensory information probabilistically, in the form of probability distributions. Several computational schemes have recently been proposed for how this might be achieved in populations of neurons. Neurophysiological data on the hypothesis, however, is almost non-existent. A major challenge for neuroscientists is to test these ideas experimentally, and so determine whether and how neurons code information about sensory uncertainty.}
}


@article{Bandits,
title = {An empirical evaluation of active inference in multi-armed bandits},
journal = {Neural Networks},
volume = {144},
pages = {229-246},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021003233},
author = {Dimitrije Marković and Hrvoje Stojić and Sarah Schwöbel and Stefan J. Kiebel},
keywords = {Decision making, Bayesian inference, Multi-armed bandits, Active inference, Upper confidence bound, Thompson sampling},
abstract = {A key feature of sequential decision making under uncertainty is a need to balance between exploiting—choosing the best action according to the current knowledge, and exploring—obtaining information about values of other actions. The multi-armed bandit problem, a classical task that captures this trade-off, served as a vehicle in machine learning for developing bandit algorithms that proved to be useful in numerous industrial applications. The active inference framework, an approach to sequential decision making recently developed in neuroscience for understanding human and animal behaviour, is distinguished by its sophisticated strategy for resolving the exploration–exploitation trade-off. This makes active inference an exciting alternative to already established bandit algorithms. Here we derive an efficient and scalable approximate active inference algorithm and compare it to two state-of-the-art bandit algorithms: Bayesian upper confidence bound and optimistic Thompson sampling. This comparison is done on two types of bandit problems: a stationary and a dynamic switching bandit. Our empirical evaluation shows that the active inference algorithm does not produce efficient long-term behaviour in stationary bandits. However, in the more challenging switching bandit problem active inference performs substantially better than the two state-of-the-art bandit algorithms. The results open exciting venues for further research in theoretical and applied machine learning, as well as lend additional credibility to active inference as a general framework for studying human and animal behaviour.}
}

@article{AIF-D,
  author    = {Noor Sajid and
               Philip J. Ball and
               Karl J. Friston},
  title     = {Demystifying active inference},
  journal   = {CoRR},
  volume    = {abs/1909.10863},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.10863},
  eprinttype = {arXiv},
  eprint    = {1909.10863},
  timestamp = {Fri, 27 Sep 2019 13:04:21 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-10863.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@book{Cherniak,
	author = {Christopher Cherniak},
	year = {1986},
	publisher = {MIT Press},
	title = {Minimal Rationality}
}

@book{Newell-Simon,
	author = {Allan Newell and Herbert Simon}, 
	year = {1972},
	title = {Human problem solving},
	publisher = {Prentice-Hall}
}
