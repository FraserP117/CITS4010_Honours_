% \documentclass[12pt, oneside]{article}
\documentclass[onecolumn]{IEEEtran}
\usepackage[
backend=biber,
style=numeric,
]{biblatex}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\graphicspath{ {images/} }
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{gensymb}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usepackage[a4paper, width=150mm, top=25mm, bottom=25mm]{geometry}
\addbibresource{references.bib}


\begin{document}

\begin{titlepage}
	\begin{center}
	
	\vspace*{0.5cm}
	
	\Huge
	\textbf{Literature Review}
	
	\vspace{0.5cm}
	\Large
	An Analysis of Active Inference and Reinforcement Learning Paradigms in Partially Observable Environments.
	
	\vspace{1.5cm}

	\textbf{Fraser Paterson}

	\vspace{1.5cm}

	A review of the extant literature, pursuant to the requirements of the\\ 
	Degree: Bachelor of Science (Honours).  
	
	\vspace{2.0cm}

	\includegraphics[width=0.4\textwidth]{UWA_Logo.png}
	
	\vspace{2.0cm}	
	
	\Large
	Supervisor: Dr Tim French\\ 
	Department of Computer Science and Software Engineering\\
	The University of Western Australia\\
	24 April 2023
	\end{center}
\end{titlepage}


\tableofcontents

\cleardoublepage

\section{Overview}
This section will be an overview of the Lit review. Can possibly put some information related to the project in here.

\section{Introduction}

\subsection{Agent Based Artificial Intelligence}
The field of Agent-based AI is the discipline concerned with the creation of autonomous systems, capable of dynamically adapting to the constraints of some environment. From the persepctive of Artificial Intelligence, an ``agent'' simply refers to any computationally instantiated entity capable of percieving and acting in its environment. Agents typically have a constitutive goal of maintaining their self-organisation across time.  

The general paradigm of interest with these methods is that of the ``sensorimotor loop''

One of the most general distinctions to be made in this field is that between ``model based'' and ``model free'' methods. 

\vspace{12pt}
\subsubsection{Model Based Methods}

\vspace{12pt}
\subsubsection{Model Free Methods}

\subsection{Generative Artificial Intelligence}
One of the most fundamental tasks that an agent can perform is \textit{prediction}.

\subsection{Partially Observable Markov Decision Porcesses (POMDPs)}

\section{Active Inference}

\subsection{An Overview}

It is highly plausible that the field of adaptive, agent-based AI is perhaps, entering its Renaissance. The last few years have made histroy, in terms of the the capability, sophistication and operation of what these methods have been able to achieve.

Active Inference is an emerging first-principles, account of adaptive behavior. Originating from Neuroscience: \textcite{A_FEP_For_The_Brain}, \textcite{The-Bayesian-Brain} and \textcite{Action-Behaviour-FE} as a corollary of the ``Free Energy Principle'', the theory is increasingly making inroads into Machine Learning and Artificial Intelligence: \textcite{RL-or-AIF} and \textcite{Applications-of-FEP-Machine-Learning-Neuroscience}. 

Active inference is a highly ambitious theory, as it purports to offer a fully unified account of action, perception and learning: \textcite{FEP-Unified-Brain-Theory}. In Active Inference, these three cognitive processes are integrated under the rubric of variational inference via the minimization of variational free energy. 

The basic postulate of the theory is that adaptive systems like living organisms, will act to fulfill prior expectations or ``preferences'' which encode desirable states for that system. These agents make use of a generative model to afford predictions about hidden states of their environment. These predictions are, in turn, used to produce inferences about the causes of observations and also to inform action-selection. 

Under Active Inference, perception is cast as the problem of inferring the hidden states of the agent's environment, given the agent's sensory observations. Action-selection and planning can be described as inference on policies over trajectories into the future. Finally, learning is viewed as inference over the parameters of the agent's generative model, that best account for the agent's sensory data. ``Learning'' of this kind takes place over a slower timescale than does perceptual inference. Active Inference describes all three processes in terms of variational free energy minimization over a single functional. This functional is composed of the agent's beliefs about the hidden states of the environment, in addition to the sensory surprisal of its observations. It should be noted that this ``divergence minus evidence'' formulation of the free energy is simply one of many, there are several other ways to parameterize the free energy, such as ``energy minus entropy'' or ``complexity minus accuracy''. I shall adhere to the ``divergence minus evidence'' formulation for the remainder of this review...

\subsection{Why Care About Active Inference?}

Active Inference addresses a problem which has plagued value-function formulations of adaptivity since their inception. This is the issue of sample-efficiency and of learning in the presence of sparse rewards: \textcite{RLflawed} and \textcite{RL-Real-World-Challenges}. If all that is available to the agent for action-selection, is a value function mapping from states and/or actions to an extrinsic reward signal, it is necessary to observe a great deal many state or state-action pairs to learn the optimal mapping from state/state-action to reward signal. For problems with large state-action spaces, this presents a significant challenge to such methods. Active Inference eschews this issue by placing ``information-gain'' on the same footing as value maximization and hence drives the agent to dynamically trade-off attention between these two goals. 

Since the Free Energy is an upper bound on sensory surprisal, and since the minimization of free energy is the sole imperative under Active inference, both action and perception have as their effect, the reduction of sensory surprisal. This is a crucial point of difference between Active Inference and Reinforcement Learning agents. While Reinforcement Learning agents select actions in an attempt to maximize a reward function of states (or states and actions), Active Inference agents select actions so as to minimize a free energy functional composed of the following two parts: 

\begin{enumerate}
	\item The discrepancy - KL divergence - between the agent's prior preference for a certain observation, and the actual observation garnered as a consequence of performing a certain action.
	\item The ``surprisal'' - negative log probability - of the observation. This is essentially a measure of how unlikely the observation is given the agent's model.
\end{enumerate}

This formulation of the Active Inference agent's objective function might appear to be unnecessarily convoluted, especially since these agents have essentially the same goal as their Reinforcement Learning cousins: adaption to the constraints of some environment. There is, however, something very special about this formulation. In addition to a ``pragmatic'' or ``reward-maximizing'' imperative, encoded by the divergence between the agent's preferred and actual observations, the free energy functional affords an additional ``epistemic'' imperative for information-gain, encoded via the surprisal. This means that Active Inference agents have a built-in affordance for exploratory/curious behavior, in addition to that of maximizing the extrinsic value of realizing their preferred observations.

Mere extrinsic, value-maximization is typically the \textit{only} operative imperative in Reinforcement Learning approaches; \textcite{Reinforcement-Learning-An-Introduction}. It is not impossible to endow Reinforcement Learning agents with a drive toward information-gain or ``curiosity''. Though typically, one has to contrive some \textit{hd hoc} manipulation of the reward signal to hand-craft an \textit{encoding} of the epistemic imperative: \textcite{Curiosity-Driven-RL}. In a nutshell, Active Inference replaces value functions with variational free energy functionals of Bayesian beliefs. Active Inference agents therefore have a built-in affordance for dynamically trading-off pragmatic and epistemic imperatives. This means that in the face of sparse rewards, or a persistent failure to realize prior preferences, Active Inference agents will naturally engage in exploratory behavior, in an attempt to find as-yet unknown routes to realize its prior preferences. 

The primary appeal of the Active Inference formulation of intelligence/adaptive Behaviour is twofold. First, it unifies the study of action, perception and planning under a single imperative, that of minimizing variational - or expected - free energy. This is an efficient formulation of these problems, since one need only address a single methodological principle instead of three. Parsimony of this kind is always desirable in any scientific theory - all things being equal - since the generality of a theory is very often a good measure of its predictive power. Second, as just elaborated, Active Inference affords a much greater sample-efficiency than does Reinforcement Learning: \textcite{Scaling-AIF}. This is an especially intriguing aspect of the theory, since the majority of real-world problems are characterized by a sparsity of rewards. Hence Active Inference is plausibly posed to inaugurate a new era of real-world agent-based AI.

Lastly, \textcolor{red}{Active inference is interesting because of its promise to be such a general method and indeed owing to a growing body of empirical research to suggest that free-energy minimization is what the brain is doing. Since the brain is thought to be the seat of ``natural'' intelligence, evidence attesting to the brain's function as a ``free-energy minimizing machine'' must surely be of interest to we who are concerned with generating instances of intelligence, artificially.}...

\subsection{Active Inference: Key Concepts} 

\vspace{12pt}
\subsubsection{Variational Inference}
Variational inference: \textcite{Variational-Inference-Reviews} is a technique of approximate Bayesian inference, in the case that the exact inference procedure becomes intractable, typically due to the large or infinite number of states over which it is necessary to marginalize. The goal in all Bayesian inference methods is to compute the posterior distribution over some variable, given the prior and likelihood distributions. These latter encode - respectively - the supposed causal relationship between the variable of interest and a observation, in addition to a ``prior'' belief about the distribution of the variable at issue. If we wish to infer the posterior distribution of $x$ as a consequence of observing $y$, then this is given as the distribution: $p(x | y)$ and Bayes's rule gives: 

$$ p(x | y) = \frac{p(y | x)p(x)}{p(y)} $$

\textcolor{red}{Not sure if this is really necesary, I should probably stick to a verbal exposition.}

For all but the smallest problems, computing the posterior exactly is intractable, due to the sheer number of states over which it is necessary to sum/integrate when computing $p(y)$. To eschew this, variational inference proposes an approximation scheme, whereby a family of ``variational'' or ``approximate'' posterior distributions is posited, each of which is a potential, approximate solution to the true posterior. These approximate posteriors are usually denoted: $q_i(x)$ where the subscript denotes that this is the i-th member of the family. We select a particular approximate posterior by choosing the one with the smallest ``divergence'' from the true posterior. Typically this is the Kullback-Libeler Divergence (KL Divergence)...

\vspace{12pt}
\subsubsection{The Free Energy Principle}

\textbf{A couple of words like this might still be appropriate?}

Historically speaking, Active Inference is a derivative of the ``Free Energy Principle'', which is a theoretical principle thought to plausibly offer a unified, constitutive account of brain function: \textcite{FEP-Rough-Guide-Brain} and perhaps even of life itself: \textcite{Life-As-We-Know-It}. 

\subsection{Present Questions}

Although Active Inference offers several exciting new directions in agent-based AI and might hold the key to truly sample-efficient real-world implementations, the theory has typically only been implemented on relatively trivial problem instances, with a small number of states and/or actions, most commonly in a discrete setting: \textcite{Uncertainty_Epistemics_AIF_Saccad} and \textcite{AIF-Epistemic-Value}. Although it has been used to great effect in these ``proof of concept'' cases, it is not yet applicable to the same sorts of problems that reinforcement learning can currently address. Many of the sorts of real-world applications of practical interest are thus far beyond the reach of Active Inference, especially for real-time implementations. The main difficulty stems from the fact that the variational free energy is a functional of hidden states and observations. Hidden states are usually exist in a very high dimensional space and observations are usually highly time-varying. The task of minimizing a highly dimensional, time-varying functional is non-trivial. The goal of scaling up the method to problems with larger state and/or action spaces, such as in the continuous case, is very much an open one, and one that must be satisfactorily addressed if Active Inference is to become a serious contender as a real-world method.  

\section{Previous Work}

Naturally, the question of scaling Active Inference to larger and more complicated state-action spaces has already begun to occupy the attention of the research community, though these endeavors are still very much in their infancy. As of the time of writing this review, two distinct approaches seem to have crystalized in the literature. The most general bifurcation is between sample-based approximation methods and distribution-based message passing methods. The latter can be either exact or approximate.

The former, sample-based methods typically make use of a function approximator such as a neural network, to parameterize the distributions of interest. This is perhaps a more``traditional'' approach, very much inspired by the success of such methods in scaling Reinforcement Learning to larger state-action spaces: \textcite{ATARI-Deep-RL}.

The latter message-passing methods take a completely different approach to the sampling paradigm. This approach represents the generative model as a ``Forney-style'' factor graph: \textcite{Codes-on-Graphs} and inference is performed via a kind of message-passing on this graph.  
Instead of processing samples, the factor graph approach manipulates full distributions to produce messages which are passed around the graph. This is an extremely fast and efficient method of implementing variational inference as it completely eschews the computational expense of processing samples, which is very often the bottleneck in sample-based approximations. 

Both approaches have their respective advantages and disadvantages. I'll now turn to a detailed exposition of each approach. 

\subsection{Generalised Filtering and the Laplace Approximation}

Generalised coordinates of motion: \textcite{Generalized-Filtering} and \textcite{Bayes-State-Estimation}.

\subsection{Factor Graphs and Message Passing Methods}

A recent and particularly novel approach that has enjoyed great success as of late, casts the problem of inference as a task of message passing updates on a Forney factor graph: \textcite{Factor-Graph-Approach-Automated-Design-Bayesian-Algos}, \textcite{Simulating-AIF-By-Message-Passing}, \textcite{Factor-Graph-Desc-Deep-Temp-AIF} and \textcite{Reactive-MP}. 

In this framework, the agent's generative model is constructed in such a was as to instantiate a Forney or ``Normal'' factor graph: \textcite{Codes-on-Graphs} and \textcite{Intro-to-Factor-Graphs}. Free Energy minimization is then cast as a process of message passing over this factor graph. 

\vspace{12pt}
\subsubsection{Forney-Style Factor Graphs (FFGs)}
A Forney-style factor graph is a graphical representation of a factorised probabilistic model. In these graphs, edges represent variables and vertices represent relationships between these variables. Consider a simple generative model - included in \textcite{Factor-Graph-Approach-Automated-Design-Bayesian-Algos}:


\begin{equation}
p(x_1, ..., x_5) = p_a(x_1) \cdot p_b(x_1, x_2) \cdot p_c(x_2, x_3, x_4) \cdot p_d(x_4, x_5) \label{eq:factorized-model}
\end{equation}

Where $p_\cdot(\cdot)$ represents a probability density function.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[>=stealth, node distance=1.5cm]
	  % Define the nodes
	  \node[draw] (pa) {$p_a$};
	  \node[draw, right=of pa] (pb) {$p_b$};
	  \node[draw, right=of pb] (pc) {$p_c$};
	  \node[draw, below=of pc] (pd) {$p_d$};

	  % Draw the edges
	  \draw[->, shorten >=2pt, shorten <=2pt] (pa) -- node[above] {$x_1$} (pb);
	  \draw[->, shorten >=2pt, shorten <=2pt] (pb) -- node[above] {$x_2$} (pc);
	  \draw[->, shorten >=2pt, shorten <=2pt] (pc) -- +(1cm, 0) node[midway, above] {$x_3$};
	  \draw[->, shorten >=2pt, shorten <=2pt] (pd) -- +(0, -1cm) node[midway, right] {$x_5$};
	  \draw[->, shorten >=2pt, shorten <=2pt] (pc) -- (pd) node[midway, right] {$x_4$};
  \end{tikzpicture}
  \caption{An FFG representation of Equation \ref{eq:factorized-model}. Adapted from \textcite{Factor-Graph-Approach-Automated-Design-Bayesian-Algos}. A node (factor) connects to all edges (variables) that are arguments in that factor. For instance, $p_c$ is connected to $x_2$, $x_3$ and $x_4$, since these all appear as arguments to factor $p_c$. Variables that only appear in one factor are represented as half-edges. Now an FFG is technically an undirected graph, however we can specify a direction for the edges to indicate the generatve direction of the model.}
  \label{fig:factor-graph}
\end{figure}


\vspace{12pt}
\subsubsection{Message Passing}

Upon observing some particular value of one of the variables, say $x_5 = \hat{x_5}$, suppose we are interested in computing the marginal posterior probability distribution of $x_2$ given this observation of $x_5$. In an FFG formulation, observing a value for a particular variable leads to the introduction of a new factor in the model. This has the effect of ``clamping'' the varaible to its observed value. Hence in our example, we now have:

$$ p(x_1, ..., x_5) \cdot \delta(x_5 - \hat{x_5})$$ 

To compute the marginal posterior distribution of $x_2$ given an observation of $x_5 = \hat{x_5}$ we simply integrate the extended model over all variables except $x_2$ and renormalze:

\begin{align}
	p(x_2 | x_5 = \hat{x}_5) &\propto \int \dots \int p(x_1, \dots, x_5) \cdot \delta(x_5 - \hat{x}_5) \, dx_1 dx_3 dx_4 dx_5 \label{eq:posterior_1} \\
	&= \int p_a(x_1) p_b(x_1, x_2) \, dx_1 \cdot \int\int p_c(x_2, x_3, x_4) \cdot \left(\int p_d(x_4, x_5) \cdot \delta(x_5 - \hat{x}_5) \, dx_5 \right) \, dx_3 dx_4 \label{eq:posterior_2}
\end{align}


These nested integrals in \eqref{eq:posterior_2} result from the substitution of the factored form of Equation \ref{eq:factorized-model} into \eqref{eq:posterior_1} and then rearranging the resulting integrals via the distributive law.

The structure of the FFG can automate the the rearrangement of these integrals into a product of nested sub-integrals. The solutions to these sub-integrals can be interpreted as mesages flowing over the FFG, hence this method is known as \textit{message passing}. The massages are ordered or ``scheduled'' so as to only contain backward dependencies. In other words, each message can be derived from preceeding messages in the schedule. Importantly, these schedules can be automatically generated by performing a depth-first search on the FFG - for isntance. 

Message passing is very efficient, since the computation of every message is local to each node in the FFG. Indeed, the message flowing from factor node $p_b$ can be derived from the mere analytic expression for $p_b$ and all messages inbound to $p_b$. Furtheremore, if the analytic form of eaach incomming message is known, a pre-derived message computation rule can be used to derive the outgoing message. These rules can be easily stored in a lookup table for resue in any model in which that specific factor-message combination is found. 

The above example elaborates the fundaments of the \textit{sum-product} message passing algorithm, however various message passing algorithms exist, such as Variational Message Passing. All message passing schemes greatly reduces the number of terms over which it is necessary to sum, when computing the approximate marginal and posterior distributions; affording much more efficient inference and a great potential for scaling up to larger state-action spaces. Indeed, this method does not make use of any approximation by means of a sampling procedure and so it avoids the computational burden associated with calculating these samples. 

Since this method relies upon a particular schedule of message-passing update rules on the underlying factor graph, all functions used need to be invertible and an inference is performed via a closed form update where the prior and likelihood distributions must be conjugate. The model passes around full distributions instead of mere samples. This results in a very fast and efficient implementation - when applicable, but the issue is that it is not a completely generic method, owing to the many assumptions as to the model structure just enumerated. many real-world distributions do not have conjugate prior and likelihoods and so this method cannot be applied in these cases.


\subsection{Sampling Based Approximation Methods}

A more standard approach that has seen a comparatively greater deal of attention is that of using deep neural network function approximators to either parameterize the distributions of interest, or to afford an efficient means of sampling these distributions. This affords approximate inference. Indeed this "genre" of approach has already seen great success in scaling reinforcement learning methods to larger state-action spaces: \textcite{Async-Methods-Deep-RL}, \textcite{ATARI-Deep-RL}, other citations needed and available.

See: \textcite{Deep-AIF}, \textcite{Applications-of-FEP-Machine-Learning-Neuroscience}, \textcite{Deep-AIF-As-Var-Policy-Grad}, \textcite{Reinforcement-Learning-Through-AIF} and \textcite{Bayesian-Policy-Selection-Using-AIF}. 

Of particular interest are: \textcite{Scaling-AIF}, \textcite{Bayesian-Policy-Selection-Using-AIF} and \textcite{Contrastive-AIF}. The former makes use of amortized inference, in the form of neural network function approximators to parameterize the relevant distributions. Free Energy minimization is then performed with respect to the function approximators. In addition, the free energy functional is amortized over the training data. This affords several advantages. For example, the number of parameters remains constant with respect to the size of the data and inference can be achieved via a single forward pass through the network. This contrasts with the iterative approach, where the VFE must be scored for every sample, individually. The resulting algorithm was able to explore a much greater proportion of the state space in a simple stationary environment, in comparison with two Reinforcement Learning baseline agents. In addition, the agent was able to learn to control the continuous inverted pendulum task with a far greater sample efficiency than the baseline agents. Although the approach offered in \textcite{Scaling-AIF} is promising, its analysis was restricted in every case to fully observable environments. This potentially sold the implementaon short, since the partially observable domain is the more "natural" problem instance for which active inference was conceived as a solution strategy. Active Inference has a built-in drive to effect uncertainty reduction, this is not so with standard reinforcement learning, for which only ad-hoc strategies exist to afford the same sort of epistemic drive that exists in active inference. This is a salient point of departure between active inference and reinforcement learning, since both already implement strategies for realizing pragmatic value. 

Pragmatic value is encoded as the sum of discounted reward across time, in the case of reinforcement learning. In the case of active inference, the drive to realize pragmatic value is afforded by the choice of action/s that realize the agent's prior preferences. The realization of prior preferences in active inference is analogous to maximizing the reward signal in reinforcement learning. However there is no analogous process for a drive to realize epistemic value, going from active inference to reinforcement learning, at least not without some aforementioned ad-hoc contrivance of the reward signal. 

Lastly, the approach of \textcite{Contrastive-AIF}, implemented a contrastive method for their Active Inference agent, which significantly reduced the computational burden of learning the parameters for the generative model and planning future actions. This method performed substantially better than the usual, likelihood-based ``reconstructive'' means of implementing Active Inference and it was also computationally cheaper to train. Importantly, this method offered a unique way to afford increased model-robustness in the face of environmental distractors. 

Main disadvantage of these methods is the computational resources involved in the sampling procedure...

\section{Gaps in The Literature}
Though there has been much focus on the implementation of active inference methods for small, discrete state-action spaces: \textcite{Applications-of-FEP-Machine-Learning-Neuroscience}, \textcite{AIF-Discrete-Action-Spaces-Synthesis}, \textcite{Step-by-Step-Tutorial-AIF-Empirical-Data}, \textcite{Relationship-Dynamic-Programming-AIF} and \textcite{AIF-Epistemic-Value}. The method is not currently viable for practical use in larger or continuous state-action spaces, for which it is necessary to plan future actions over some time horizon. Owing to the relatively small size of the state-action spaces in which active inference has historically been implemented, it has been possible to simply evaluate the expected free energy of all possible actions over the specified time horizon. This owes primarily to the issue of evaluating the expected free energy, which is the expectation of the Variational free energy evaluated for future actions over some time horizon: \textcite{Message-Passing-Perspective-Planning-Under-AIF} and \textcite{Bayesian-Policy-Selection-Using-AIF}. 

Unfortunately, enumerating all possible action-trajectories over the specified time horizon does not scale well to problems with larger state-action spaces
and/or longer time horizons. Hence we can now specify exactly what it is that the problem of ``scaling'' is supposed to be. 

\textcolor{red}{Let $S$ be a solution technique. Let $P_1$ and $P_2$ be problem instances of the same type. Let $X$ and $Y$ be the solution spaces for $P_1$ and $P_2$ (respectfully), where $|X| << |Y|$. Suppose the solution technique $S$ affords an adequate solution to problem instance $P_1$, in the sense that the solution is both adequate for the task at hand and $S$ found the solution in an adequate amount of time, consuming an acceptable amount of computational resources.}

\textcolor{red}{$S$ will be is said to scale (or scale well) to problem instance $P_2$, if $S$ can generate a solution to $P_2$, in an acceptable amount of time, while consuming an acceptable amount of computational resources. In other words, the cost associated with generating the solution to $P_2$ does not outweigh the utility of being able to generate the solution to $P_2$, $S$ is a ``viable'' solution technique for instance $P_2$.}

Evaluating all possible trajectories in a problem instance's state-action space, scales exponentially with the size of the state-action space: \textcite{Applications-of-FEP-Machine-Learning-Neuroscience}. For large state-action spaces, evaluating all possible action trajectories quickly becomes an ``unviable'' solution technique.

\section{Discussion}
\textbf{In this section, I'll aim to settle on a particular ``gap'' as identified above and to justify my choice in this regard.}

\section{Conclusion}

Here I think I'll reiterate why this problem of scaling active inference is important at all and suggest potential implications for being able to make some headway in on this problem. 

\appendix
\section{Appendix}

% \subsection{Partially Observable Markov Decision Process (POMDP)}
% This is important enough to the topic that I think it might just need to go in the text, probably in the introduction.

\subsection{Factor Graphs}

\subsection{Message Passing}

% \subsection{Model Based Methods}

% \subsection{Model Free Methods}

Remaining things that will some elaboration, or at least a definition?

\begin{enumerate}
	\item Bayesian Inference
	\item Policy (Reinforcement Learning vs Active Inference framing)
	\item Amortized Inference
\end{enumerate}


\printbibliography

\end{document}
