% \documentclass[12pt, oneside]{article}
\documentclass[onecolumn]{IEEEtran}
\usepackage[
backend=biber,
style=numeric,
]{biblatex}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\graphicspath{ {images/} }
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{gensymb}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usepackage[a4paper, width=150mm, top=25mm, bottom=25mm]{geometry}
\addbibresource{references.bib}


\begin{document}

\begin{titlepage}
	\begin{center}
	
	\vspace*{0.5cm}
	
	\Huge
	\textbf{Literature Review}
	
	\vspace{0.5cm}
	\Large
	An Analysis of Active Inference and Reinforcement Learning Paradigms in Partially Observable Environments.
	
	\vspace{1.5cm}

	\textbf{Fraser Paterson}

	\vspace{1.5cm}

	A review of the extant literature, pursuant to the requirements of the\\ 
	Degree: Bachelor of Science (Honours).  
	
	\vspace{2.0cm}

	\includegraphics[width=0.4\textwidth]{UWA_Logo.png}
	
	\vspace{2.0cm}	
	
	\Large
	Supervisor: Dr Tim French\\ 
	Department of Computer Science and Software Engineering\\
	The University of Western Australia\\
	24 April 2023
	\end{center}
\end{titlepage}


\tableofcontents

\cleardoublepage

\section{Overview}
This section will be an overview of the Lit review. Can possibly put some information related to the project in here.

\section{Introduction}

\subsection{Agent Based Artificial Intelligence}
The field of Agent-based AI is the discipline concerned with the creation of autonomous systems, capable of dynamically adapting to the constraints of some environment. From the perspective of Artificial Intelligence, an ``agent'' simply refers to any computationally instantiated entity capable of perceiving and acting in its environment. Agents typically have a constitutive goal of maintaining their self-organization across time.  

The general paradigm of interest with these methods is that of the ``sensorimotor loop''

One of the most general distinctions to be made in this field is that between ``model based'' and ``model free'' methods. 

\vspace{12pt}
\subsubsection{Model Based Methods}

\vspace{12pt}
\subsubsection{Model Free Methods}

\subsection{Generative Artificial Intelligence}
One of the most fundamental tasks that an agent can perform is \textit{prediction}.

\subsection{Partially Observable Markov Decision Processes (POMDPs)}

\section{Active Inference}

\subsection{An Overview}

It is highly plausible that the field of adaptive, agent-based AI is perhaps, entering its Renaissance. The last few years have made history, in terms of the the capability, sophistication and operation of what these methods have been able to achieve.

Active Inference is an emerging first-principles, account of adaptive behavior. Originating from Neuroscience: \textcite{A_FEP_For_The_Brain}, \textcite{The-Bayesian-Brain} and \textcite{Action-Behaviour-FE} as a corollary of the ``Free Energy Principle'', the theory is increasingly making inroads into Machine Learning and Artificial Intelligence: \textcite{RL-or-AIF} and \textcite{Applications-of-FEP-Machine-Learning-Neuroscience}. 

Active inference is a highly ambitious theory, as it purports to offer a fully unified account of action, perception and learning: \textcite{FEP-Unified-Brain-Theory}. In Active Inference, these three cognitive processes are integrated under the rubric of variational inference via the minimization of variational free energy. 

The basic postulate of the theory is that adaptive systems like living organisms, will act to fulfill prior expectations or ``preferences'' which encode desirable states for that system. These agents make use of a generative model to afford predictions about hidden states of their environment. These predictions are, in turn, used to produce inferences about the causes of observations and also to inform action-selection. 

Under Active Inference, perception is cast as the problem of inferring the hidden states of the agent's environment, given the agent's sensory observations. Action-selection and planning can be described as inference on policies over trajectories into the future. Finally, learning is viewed as inference over the parameters of the agent's generative model, that best account for the agent's sensory data. ``Learning'' of this kind takes place over a slower timescale than does perceptual inference. Active Inference describes all three processes in terms of variational free energy minimization over a single functional. This functional is composed of the agent's beliefs about the hidden states of the environment, in addition to the sensory surprisal of its observations. It should be noted that this ``divergence minus evidence'' formulation of the free energy is simply one of many, there are several other ways to parameterize the free energy, such as ``energy minus entropy'' or ``complexity minus accuracy''. I shall adhere to the ``divergence minus evidence'' formulation for the remainder of this review...

\subsection{Why Care About Active Inference?}

Active Inference addresses a problem which has plagued value-function formulations of adaptivity since their inception. This is the issue of sample-efficiency and of learning in the presence of sparse rewards: \textcite{RLflawed} and \textcite{RL-Real-World-Challenges}. If all that is available to the agent for action-selection, is a value function mapping from states and/or actions to an extrinsic reward signal, it is necessary to observe a great deal many state or state-action pairs to learn the optimal mapping from state/state-action to reward signal. For problems with large state-action spaces, this presents a significant challenge to such methods. Active Inference eschews this issue by placing ``information-gain'' on the same footing as value maximization and hence drives the agent to dynamically trade-off attention between these two goals. 

Since the Free Energy is an upper bound on sensory surprisal, and since the minimization of free energy is the sole imperative under Active inference, both action and perception have as their effect, the reduction of sensory surprisal. This is a crucial point of difference between Active Inference and Reinforcement Learning agents. While Reinforcement Learning agents select actions in an attempt to maximize a reward function of states (or states and actions), Active Inference agents select actions so as to minimize a free energy functional composed of the following two parts: 

\begin{enumerate}
	\item The discrepancy - KL divergence - between the agent's prior preference for a certain observation, and the actual observation garnered as a consequence of performing a certain action.
	\item The ``surprisal'' - negative log probability - of the observation. This is essentially a measure of how unlikely the observation is given the agent's model.
\end{enumerate}

This formulation of the Active Inference agent's objective function might appear to be unnecessarily convoluted, especially since these agents have essentially the same goal as their Reinforcement Learning cousins: adaption to the constraints of some environment. There is, however, something very special about this formulation. In addition to a ``pragmatic'' or ``reward-maximizing'' imperative, encoded by the divergence between the agent's preferred and actual observations, the free energy functional affords an additional ``epistemic'' imperative for information-gain, encoded via the surprisal. This means that Active Inference agents have a built-in affordance for exploratory/curious behavior, in addition to that of maximizing the extrinsic value of realizing their preferred observations.

Mere extrinsic, value-maximization is typically the \textit{only} operative imperative in Reinforcement Learning approaches; \textcite{Reinforcement-Learning-An-Introduction}. It is not impossible to endow Reinforcement Learning agents with a drive toward information-gain or ``curiosity''. Though typically, one has to contrive some \textit{hd hoc} manipulation of the reward signal to hand-craft an \textit{encoding} of the epistemic imperative: \textcite{Curiosity-Driven-RL}. In a nutshell, Active Inference replaces value functions with variational free energy functionals of Bayesian beliefs. Active Inference agents therefore have a built-in affordance for dynamically trading-off pragmatic and epistemic imperatives. This means that in the face of sparse rewards, or a persistent failure to realize prior preferences, Active Inference agents will naturally engage in exploratory behavior, in an attempt to find as-yet unknown routes to realize its prior preferences. 

The primary appeal of the Active Inference formulation of intelligence/adaptive Behaviour is twofold. First, it unifies the study of action, perception and planning under a single imperative, that of minimizing variational - or expected - free energy. This is an efficient formulation of these problems, since one need only address a single methodological principle instead of three. Parsimony of this kind is always desirable in any scientific theory - all things being equal - since the generality of a theory is very often a good measure of its predictive power. Second, as just elaborated, Active Inference affords a much greater sample-efficiency than does Reinforcement Learning: \textcite{Scaling-AIF}. This is an especially intriguing aspect of the theory, since the majority of real-world problems are characterized by a sparsity of rewards. Hence Active Inference is plausibly posed to inaugurate a new era of real-world agent-based AI.

Lastly, Active inference is interesting owing to a growing body of empirical research to suggest that free-energy minimization is the primary imperative of brain function: \textcite{FEP-Rough-Guide-Brain}, \textcite{The-Bayesian-Brain}, \textcite{Active-Inference-Book}, \textcite{Neural-Dynamics-AIF} and \textcite{AIF-A-Process-Theory}, to scratch the surface. Since the brain is thought to be the seat of ``natural'' intelligence, evidence attesting to the brain's function as a ``free-energy minimizing machine'' must surely be of interest to we who are concerned with generating instances of intelligence, artificially.

\subsection{Active Inference: Key Concepts} 

\vspace{12pt}
\subsubsection{Variational Inference}
Variational inference: \textcite{Variational-Inference-Reviews} is a technique of approximate Bayesian inference, in the case that the exact inference procedure becomes intractable, typically due to the large or infinite number of states over which it is necessary to marginalize. The goal in all Bayesian inference methods is to compute the posterior distribution over some variable, given the prior and likelihood distributions. These latter encode - respectively - the supposed causal relationship between the variable of interest and a observation, in addition to a ``prior'' belief about the distribution of the variable at issue. If we wish to infer the posterior distribution of $x$ as a consequence of observing $y$, then this is given as the distribution: $p(x | y)$ and Bayes's rule gives: 

$$ p(x | y) = \frac{p(y | x)p(x)}{p(y)} $$

For all but the smallest problems, computing the posterior exactly is intractable, due to the sheer number of states over which it is necessary to sum/integrate when computing $p(y)$. To eschew this, variational inference proposes an approximation scheme, whereby a family of ``variational'' or ``approximate'' posterior distributions is posited, each of which is a potential, approximate solution to the true posterior. These approximate posteriors are usually denoted: $q_i(x)$ where the subscript denotes that this is the i-th member of the family. We select a particular approximate posterior by choosing the one with the smallest ``divergence'' from the true posterior. Typically this is the Kullback-Libeler Divergence (KL Divergence)...

\vspace{12pt}
\subsubsection{The Free Energy Principle}

Historically speaking, Active Inference is a derivative of the ``Free Energy Principle'', which is a theoretical principle thought to plausibly offer a unified, constitutive account of brain function: \textcite{FEP-Rough-Guide-Brain} and perhaps even of life itself: \textcite{Life-As-We-Know-It}. 

\subsection{Present Questions}

Although Active Inference offers several exciting new directions in agent-based AI and might hold the key to truly sample-efficient real-world implementations, the theory has typically only been implemented on relatively trivial problem instances, with a small number of states and/or actions, most commonly in a discrete setting: \textcite{Uncertainty_Epistemics_AIF_Saccad} and \textcite{AIF-Epistemic-Value}. Although it has been used to great effect in these ``proof of concept'' cases, it is not yet applicable to the same sorts of problems that reinforcement learning can currently address. 

This is largely due to the inherently exponential search space of evaluating potential actions into some horizon into the future. Sophisticated agents require the ability to plan actions in this manner, not merely to perform the optimal action at the present time. This requires the determination of a sequence of actions (a trajectory) into the future time horizon, the ability to score this trajectory and then the selection of the optimal trajectory, with respect to the cost function. Evaluating all possible trajectories in a problem instance's state-action space, scales exponentially with the size of the state-action space: \textcite{Applications-of-FEP-Machine-Learning-Neuroscience}. This is very much the ``heart of the issue'' of scaling Active Inference. In an Active Inference context, the objective function in question for the case of planning future actions is the Expected Free Energy (EFE) and it is simply the expectation of the Variational Free Energy, under the approximate posterior for a given trajectory of actions into the future. 

Aside from the exponential search space over trajectories into the future, another difficulty stems from the fact that the variational free energy is a functional of hidden states and observations. Hidden states usually exist in a very high dimensional space and observations are usually highly time-varying. Observations can easily be on the order of milliseconds, for instance. The task of minimizing a highly dimensional, time-varying functional is non-trivial. The goal of scaling up the method to problems with larger state and/or action spaces, such as in the continuous caseis, for the above reasons, very much an open one. It is a problem that must be satisfactorily addressed if Active Inference is to become a serious contender as a real-world method.  



\section{Previous Work}

Naturally, the question of scaling Active Inference to larger and more complicated state-action spaces has already begun to occupy the attention of the research community, though these endeavors are still very much in their infancy. As of the time of writing this review, two distinct approaches seem to have crystalized in the literature. The most general bifurcation is between sample-based approximation methods and distribution-based message passing methods. The latter can be either exact or approximate.

The former, sample-based methods typically make use of a function approximator such as a neural network, to parameterize the distributions of interest. This is perhaps a more``traditional'' approach, very much inspired by the success of such methods in scaling Reinforcement Learning to larger state-action spaces: \textcite{ATARI-Deep-RL}.

The latter message-passing methods take a completely different approach to the sampling paradigm. This approach represents the generative model as a ``Forney-style'' factor graph: \textcite{Codes-on-Graphs} and inference is performed via a kind of message-passing on this graph.  
Instead of processing samples, the factor graph approach manipulates full distributions to produce messages which are passed around the graph. This is an extremely fast and efficient method of implementing variational inference as it completely eschews the computational expense of processing samples, which is very often the bottleneck in sample-based approximations. 

Both approaches have their respective advantages and disadvantages. I'll now turn to a detailed exposition of each approach. 

\subsection{The Laplace Approximation and Generalised Coordinates}

A common approximation scheme used to simplify the calculation of the free energy is the \textit{Laplace Approximation}. This consists in simply assuming that the optimal approximate posterior distribution is a Gaussian distribution. The sufficient statistics of this Gaussian then become parameters which can be optimised so as to minimise the VFE. 

It is almost always further assumed that the approximate posterior is sharply peaked about its mean value and that the constituent functions of the hidden states are smooth. This makes the integration problem of evaluating the VFE appreciably non-zero, only at the peaks. We can then use a Taylor series expansion of the constituent functions about their mean value. 

This affords a highly tractible, analytic form for the VFE but the above assumptions are fairly strong ones, many of which are highly questionable - depending on the intended use case. For instance, the assumption that the approximate posteror is strongly peaked about its mean, will generally be appropriate in proportion to the degree that the approximate posterior \textit{already} constitutes a good aproximation to the true posterior. In the case of a greater amount of uncertainty about the fitness of the approximate posterior, this assumption is less and less appropriate. For small or simple problems, this is often not a large issue, but in the large multivariate case, the approximate posterior need not at all resemble a multivariate Gaussian.  

This approach also has highly non-trivial implications for the interpretation of brain function. To the degree that Active Inference is used as a model of brain function, this is an obvious limitation of the approximation. See \textcite{FEP-Mathematical-Review} 

In conjunction with the Laplace appoximation, when implementing Active Inference in continuous state-action spaces, a related means of representing beliefs about trajectories through time is to use \textit{generalised coordinates of motion}. This is a rather straightforward procedure, whereby inferences are drawn, not only about each hidden state variable - $x$, say - but also regarding each successive temporal derivative of $x$: $x'$, $x''$, $x'''$ and so on. These temporal derivatives can then be used to construct an approximation to the actual trajectory. This again, is a simple means of representing the trajectory of the hidden states, in an analytically tractible way. 

Both these approaches are perfectly viable constituent means of implementing Active Inference on their own but it is common to use both simultaneously. These approximations culminate in a method called ``Generalised Filtering'': \textcite{Generalized-Filtering} and \textcite{Bayes-State-Estimation}. This is a fully online Bayesian filtering scheme that uses generalized coordinates.

While these sorts of schemes afford computationally tractible means of formuating and evaluating the VFE, they do not necessarily afford the ability to scale these procedures to larger problem instances.


\subsection{Factor Graphs and Message Passing Methods}

A recent and particularly novel approach that has enjoyed great success as of late, casts the problem of inference as a task of message passing updates on a Forney factor graph: \textcite{Factor-Graph-Approach-Automated-Design-Bayesian-Algos}, \textcite{Simulating-AIF-By-Message-Passing}, \textcite{Factor-Graph-Desc-Deep-Temp-AIF} and \textcite{Reactive-MP}. 

In this framework, the agent's generative model is constructed in such a was as to instantiate a Forney or ``Normal'' factor graph: \textcite{Codes-on-Graphs} and \textcite{Intro-to-Factor-Graphs}. Free Energy minimization is then cast as a process of message passing over this factor graph. 

\vspace{12pt}
\subsubsection{Forney-Style Factor Graphs (FFGs)}
A Forney-style factor graph is a graphical representation of a factorised probabilistic model. In these graphs, edges represent variables and vertices represent relationships between these variables. Consider a simple generative model - included in \textcite{Factor-Graph-Approach-Automated-Design-Bayesian-Algos}:


\begin{equation}
p(x_1, ..., x_5) = p_a(x_1) \cdot p_b(x_1, x_2) \cdot p_c(x_2, x_3, x_4) \cdot p_d(x_4, x_5) \label{eq:factorized-model}
\end{equation}

Where $p_\cdot(\cdot)$ represents a probability density function.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[>=stealth, node distance=1.5cm]
	  % Define the nodes
	  \node[draw] (pa) {$p_a$};
	  \node[draw, right=of pa] (pb) {$p_b$};
	  \node[draw, right=of pb] (pc) {$p_c$};
	  \node[draw, below=of pc] (pd) {$p_d$};

	  % Draw the edges
	  \draw[->, shorten >=2pt, shorten <=2pt] (pa) -- node[above] {$x_1$} (pb);
	  \draw[->, shorten >=2pt, shorten <=2pt] (pb) -- node[above] {$x_2$} (pc);
	  \draw[->, shorten >=2pt, shorten <=2pt] (pc) -- +(1cm, 0) node[midway, above] {$x_3$};
	  \draw[->, shorten >=2pt, shorten <=2pt] (pd) -- +(0, -1cm) node[midway, right] {$x_5$};
	  \draw[->, shorten >=2pt, shorten <=2pt] (pc) -- (pd) node[midway, right] {$x_4$};
  \end{tikzpicture}
  \caption{An FFG representation of Equation \ref{eq:factorized-model}. Adapted from \textcite{Factor-Graph-Approach-Automated-Design-Bayesian-Algos}. A node (factor) connects to all edges (variables) that are arguments in that factor. For instance, $p_c$ is connected to $x_2$, $x_3$ and $x_4$, since these all appear as arguments to factor $p_c$. Variables that only appear in one factor are represented as half-edges. Now an FFG is technically an undirected graph, however we can specify a direction for the edges to indicate the generative direction of the model.}
  \label{fig:factor-graph}
\end{figure}


\vspace{12pt}
\subsubsection{Message Passing}

Upon observing some particular value of one of the variables, say $x_5 = \hat{x_5}$, suppose we are interested in computing the marginal posterior probability distribution of $x_2$ given this observation of $x_5$. In an FFG formulation, observing a value for a particular variable leads to the introduction of a new factor in the model. This has the effect of ``clamping'' the variable to its observed value. Hence in our example, we now have:

$$ p(x_1, ..., x_5) \cdot \delta(x_5 - \hat{x_5})$$ 

To compute the marginal posterior distribution of $x_2$ given an observation of $x_5 = \hat{x_5}$ we simply integrate the extended model over all variables except $x_2$ and renormalze:

\begin{align}
	p(x_2 | x_5 = \hat{x}_5) &\propto \int \dots \int p(x_1, \dots, x_5) \cdot \delta(x_5 - \hat{x}_5) \, dx_1 dx_3 dx_4 dx_5 \label{eq:posterior_1} \\
	&= \int p_a(x_1) p_b(x_1, x_2) \, dx_1 \cdot \int\int p_c(x_2, x_3, x_4) \cdot \left(\int p_d(x_4, x_5) \cdot \delta(x_5 - \hat{x}_5) \, dx_5 \right) \, dx_3 dx_4 \label{eq:posterior_2}
\end{align}


These nested integrals in \eqref{eq:posterior_2} result from the substitution of the factored form of Equation \ref{eq:factorized-model} into \eqref{eq:posterior_1} and then rearranging the resulting integrals via the distributive law.

The structure of the FFG can automate the the rearrangement of these integrals into a product of nested sub-integrals. The solutions to these sub-integrals can be interpreted as messages flowing over the FFG, hence this method is known as \textit{message passing}. The massages are ordered or ``scheduled'' so as to only contain backward dependencies. In other words, each message can be derived from preceding messages in the schedule. Importantly, these schedules can be automatically generated by performing a depth-first search on the FFG - for instance. 

Message passing is very efficient, since the computation of every message is local to each node in the FFG. Indeed, the message flowing from factor node $p_b$ can be derived from the mere analytic expression for $p_b$ and all messages inbound to $p_b$. Furthermore, if the analytic form of each incoming message is known, a pre-derived message computation rule can be used to derive the outgoing message. These rules can be easily stored in a lookup table for reuse in any model in which that specific factor-message combination is found. 

The above example elaborates the fundaments of the \textit{sum-product} message passing algorithm, however various message passing algorithms exist, such as Variational Message Passing. All message passing schemes greatly reduces the number of terms over which it is necessary to sum, when computing the approximate marginal and posterior distributions; affording much more efficient inference and a great potential for scaling up to larger state-action spaces. Indeed, this method does not make use of any approximation by means of a sampling procedure and so it avoids the computational burden associated with calculating these samples. 

Since this method relies upon a particular schedule of message-passing update rules on the underlying factor graph, all functions used need to be invertible (bijective) and an inference is performed via a closed form update where the prior and likelihood distributions must be conjugate. The model passes around full distributions instead of mere samples. This results in a very fast and efficient implementation - when applicable, but the issue is that it is not a completely generic method, owing to the many assumptions as to the model structure just enumerated. many real-world distributions do not have conjugate prior and likelihoods and so this method cannot be applied in these cases. 


\subsection{Sampling Based Approximation Methods}

A more standard approach that has seen a comparatively greater deal of attention is that of using deep neural network function approximators to either parameterize the distributions of interest, or to afford an efficient means of sampling these distributions. This instantiates an approximate inference scheme. Indeed this ``genre'' of approach has already seen great success in scaling reinforcement learning methods to larger state-action spaces: \textcite{Async-Methods-Deep-RL}, \textcite{ATARI-Deep-RL} and so it is a natural choice for attempting the same task in Active Inference.

Early work in this regard was condicted by: \textcite{Scaling-AIF}. This approach uses deep neural networks to parameterise the distributions of in the generative model amortizes the inference procedure Free Energy minimization is then performed with respect to these function approximators. the use of deep neural networks to parameterise the model's distributions had been done before: \textcite{Deep-AIF} and \textcite{Deep-AIF-As-Var-Policy-Grad} but the amortization of the free energy functional over the training data was an added design choice that afforded several advantages. 

For example, the number of parameters remains constant with respect to the size of the data and inference can be achieved via a single forward pass through the network. This contrasts with the iterative approach, where the VFE must be scored for every sample, individually. The resulting algorithm was able to explore a much greater proportion of the state space in a simple stationary environment, in comparison with two Reinforcement Learning baseline agents. In addition, the agent was able to learn to control the continuous inverted pendulum task with a far greater sample efficiency than the baseline agents. Although the approach offered in \textcite{Scaling-AIF} is promising, its analysis was restricted in every case to fully observable environments. This potentially sold the implementaon short, since the partially observable domain is the more ``natural'' problem instance for which active inference was conceived as a solution strategy. \textcite{Scaling-AIF} also embedded a reward signal into the observation space and set a prior on observing high reward outcomes. This was done simply by making the reward signal an observation specific modality. 

The horizon across which \textcite{Scaling-AIF} computed the EFE was held fixed, and dennoted as $H$. $N$ sample policies are drawn from the parameterised, approximate posterior, the negative EFE is evaluated for each of these samples and then the approximate posterior is ``refit'' to the top $M$ samples. Each sample is weighted in proportion to its EFE. This procedure is carried put $I$ times, after which, the mean of the belief for the current time step is returned. This is the action selected for performance, at the current time step. \textcite{Scaling-AIF} used $N = 1000$, $M = 100$ and $I = 10$. Hence, a distribution over policies is updated after each observation.

This approach cannot capture the exact shape of the decision sequence, though agents typicaly only need to identify the peak of the EFE landscape in order to act optimally. This is perfectly acceptable for a relatively short time horizon, but this limitation becomes prohibitive in the case of a larger one. 

A more recent approach, as per: \textcite{Learn-Gen-State-Space-Models-AIF}, sucessfully learns a complex model directly from real-world pixel data. This appraoch used Deep Nerual Networks to learn the generative model from scratch, without the need to hand-craft any part of the model. Hand-crafted models are thus far the standard approach, this is obviously a time-consuming, tedious procedure. The model learned directly from observation-action sequences and constructed the agent's generative model from this raw data. In addition, the approachwas able to learn with high-dimensional pixel observations on OpenAI gym baselines. 
In contrast to \textcite{Scaling-AIF}, \textcite{Learn-Gen-State-Space-Models-AIF} did not need to specify a prior on the agent's belief space. The necessity to specify such priors can be challenging, since in complicated problems, it is not necessarily obvious what these priors should be. Similarly to \textcite{Scaling-AIF}, the EFE was estimated from sampled trajectories, effectvely implementing Active Infernce as a tree search over policies. 

While this appraoch is promising, the implementation uses a pre-recorded dataset of observation-action pairs to trainthe networks. Ideally, the model should be learnied in an online fashon, so that action and learning are interleaved, removing the necesity of the pre-recorded dataset. Somewhat ironicaly, one possible approach to afford this wouold be to maintain a posterior distribution over model parameters, similar to \textcite{Scaling-AIF}. there are several other similar approaches, such as \textcite{DEEP-AIF-For-POMDPs}, which used a Variational Autoencoder to encode the representation of continuous states and \textcite{Combine-Info-Seek-Explore-and-Reward-Maximization-Under-POMDP}, which proposed a hybrind Active Inference and Reinforcement Learning objective called ``unified inference''. In the main, all these sampling approaches use deep neural networks to finesse the issue of estimating the posterior densities at issue, and most crucially the EFE itself.

Lastly, the approach of \textcite{Contrastive-AIF}, implemented a contrastive method for their Active Inference agent, which significantly reduced the computational burden of learning the parameters for the generative model and planning future actions. This method performed substantially better than the usual, likelihood-based ``reconstructive'' means of implementing Active Inference and it was also computationally cheaper to train. Importantly, this method offered a unique way to afford increased model-robustness in the face of environmental distractors. 

the primary disadvantage of these methods is the computational resources involved in the sampling procedure itself. it is an inherent limitation of these methods that they must cumpute a large nunber of samples in order to approximate the approximate posterior. The central question at issue in this formulaton of the problem is ``how does one restrict attention to the best trajectories into the future, given the exponential search space of trajectories into the future?''

\section{Gaps in The Literature}
Though there has been much focus on the implementation of active inference methods for small, discrete state-action spaces: \textcite{Applications-of-FEP-Machine-Learning-Neuroscience}, \textcite{AIF-Discrete-Action-Spaces-Synthesis}, \textcite{Step-by-Step-Tutorial-AIF-Empirical-Data}, \textcite{Relationship-Dynamic-Programming-AIF} and \textcite{AIF-Epistemic-Value}. The method is not currently viable for practical use in larger or continuous state-action spaces, for which it is necessary to plan future actions over some time horizon. Owing to the relatively small size of the state-action spaces in which active inference has historically been implemented, it has been possible to simply evaluate the expected free energy of all possible actions over the specified time horizon. This owes primarily to the issue of evaluating the expected free energy, which is the expectation of the Variational free energy evaluated for future actions over some time horizon: \textcite{Message-Passing-Perspective-Planning-Under-AIF} and \textcite{Bayesian-Policy-Selection-Using-AIF}. 

Unfortunately, enumerating all possible action-trajectories over the specified time horizon does not scale well to problems with larger state-action spaces
and/or longer time horizons. Hence we can now specify exactly what it is that the problem of ``scaling'' is supposed to be. 

\textcolor{red}{Let $S$ be a solution technique. Let $P_1$ and $P_2$ be problem instances of the same type. Let $X$ and $Y$ be the solution spaces for $P_1$ and $P_2$ (respectfully), where $|X| << |Y|$. Suppose the solution technique $S$ affords an adequate solution to problem instance $P_1$, in the sense that the solution is both adequate for the task at hand and $S$ found the solution in an adequate amount of time, consuming an acceptable amount of computational resources.}

\textcolor{red}{$S$ will be is said to scale (or scale well) to problem instance $P_2$, if $S$ can generate a solution to $P_2$, in an acceptable amount of time, while consuming an acceptable amount of computational resources. In other words, the cost associated with generating the solution to $P_2$ does not outweigh the utility of being able to generate the solution to $P_2$, $S$ is a ``viable'' solution technique for instance $P_2$.}

Evaluating all possible trajectories in a problem instance's state-action space, scales exponentially with the size of the state-action space: \textcite{Applications-of-FEP-Machine-Learning-Neuroscience}. For large state-action spaces, evaluating all possible action trajectories quickly becomes an ``unviable'' solution technique.

\section{Discussion}
\textbf{In this section, I'll aim to settle on a particular ``gap'' as identified above and to justify my choice in this regard.}

\section{Conclusion}

Here I think I'll reiterate why this problem of scaling active inference is important at all and suggest potential implications for being able to make some headway in on this problem. 

\appendix
\section{Appendix}

Remaining things that will some elaboration, or at least a definition? I'm not sure this appendix will be necessary in the end.

\begin{enumerate}
	\item Bayesian Inference
	\item Policy (Reinforcement Learning vs Active Inference framing)
	\item Amortized Inference
\end{enumerate}


\printbibliography

\end{document}
